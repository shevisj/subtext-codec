{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ac67d",
   "metadata": {},
   "source": [
    "# Subtext Codec Demo\n",
    "\n",
    "This notebook demonstrates **subtext-codec**, a steganographic codec that hides arbitrary binary data\n",
    "inside LLM-generated text by steering token selection via logit rank.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Encoding**: Binary data is converted to a sequence of \"digits\" in a variable base. Each digit\n",
    "   determines which token rank (from the model's top-k predictions) to select at each generation step.\n",
    "   This produces natural-looking text that secretly encodes your data.\n",
    "\n",
    "2. **Decoding**: Given the encoded text and a key, we re-run the model to determine what the top-k\n",
    "   tokens were at each step. By observing which token was actually chosen, we recover the original\n",
    "   digit sequence and convert it back to bytes.\n",
    "\n",
    "The process is **fully reversible** and **deterministic** when using the same model and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we configure CUDA for deterministic behavior. This environment variable ensures\n",
    "reproducible results when using GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841813e-11d8-4b9f-8000-a3cbc08a6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA workspace configuration for deterministic cuBLAS operations.\n",
    "# This is required BEFORE importing torch to ensure reproducible GPU computations.\n",
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83062502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the subtext_codec library\n",
    "import subtext_codec\n",
    "\n",
    "# Also import json for loading/viewing the key file\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"subtext-codec version: {subtext_codec.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "samples-section",
   "metadata": {},
   "source": [
    "## 2. Explore the Sample Data\n",
    "\n",
    "The `samples/` directory contains pre-generated example files:\n",
    "\n",
    "- **`secret.txt`**: The original secret payload (Lorem ipsum text)\n",
    "- **`key.json`**: The codec key containing parameters needed for decoding\n",
    "- **`message.txt`**: The encoded text (looks like a steganography tutorial!)\n",
    "- **`decoded.txt`**: The decoded payload (should match secret.txt exactly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-samples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to sample files\n",
    "SAMPLES_DIR = Path(\"samples\")\n",
    "\n",
    "SECRET_FILE = SAMPLES_DIR / \"secret.txt\"      # Original secret data to encode\n",
    "KEY_FILE = SAMPLES_DIR / \"key.json\"           # Codec key for encoding/decoding\n",
    "MESSAGE_FILE = SAMPLES_DIR / \"message.txt\"    # Encoded steganographic text\n",
    "DECODED_FILE = SAMPLES_DIR / \"decoded.txt\"    # Decoded output (should match secret)\n",
    "\n",
    "# Verify all sample files exist\n",
    "for f in [SECRET_FILE, KEY_FILE, MESSAGE_FILE, DECODED_FILE]:\n",
    "    assert f.exists(), f\"Missing sample file: {f}\"\n",
    "print(\"All sample files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the secret payload\n",
    "# This is the data we want to hide inside LLM-generated text\n",
    "secret_data = SECRET_FILE.read_bytes()\n",
    "\n",
    "print(f\"Secret payload size: {len(secret_data)} bytes\")\n",
    "print(f\"\\n--- Secret Content ---\\n\")\n",
    "print(secret_data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the codec key\n",
    "# The key contains all parameters needed to decode the message\n",
    "with open(KEY_FILE) as f:\n",
    "    key_data = json.load(f)\n",
    "\n",
    "print(\"--- Codec Key Contents ---\\n\")\n",
    "for k, v in key_data.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n--- Key Field Explanations ---\")\n",
    "print(\"\"\"\n",
    "  version:           Codec version (v2 = dynamic mixed-radix encoding)\n",
    "  top_k:             Maximum number of token candidates at each step\n",
    "  top_p:             Nucleus sampling threshold (cumulative probability cutoff)\n",
    "  prompt_prefix:     The text prompt that precedes the encoded content\n",
    "  model_name_or_path: HuggingFace model identifier used for encoding\n",
    "  device:            Computation device (cuda/cpu)\n",
    "  torch_dtype:       Model precision (bf16 for bfloat16)\n",
    "  payload_length:    Exact byte count of original payload (for reconstruction)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "view-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the encoded message\n",
    "# Notice how it reads like natural text about steganography!\n",
    "# The secret data is hidden in the specific word choices.\n",
    "encoded_message = MESSAGE_FILE.read_text()\n",
    "\n",
    "print(f\"Encoded message size: {len(encoded_message)} characters\")\n",
    "print(f\"\\n--- Encoded Message (first 1500 chars) ---\\n\")\n",
    "print(encoded_message[:1500])\n",
    "print(\"\\n[... message continues ...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-section",
   "metadata": {},
   "source": [
    "## 3. Load the Language Model\n",
    "\n",
    "Both encoding and decoding require the same language model. The model generates probability\n",
    "distributions over tokens, which the codec uses to hide/recover data.\n",
    "\n",
    "**Note**: This step downloads and loads a large model (~16GB for Llama-3.1-8B).\n",
    "Make sure you have sufficient GPU memory and have accepted the model's license on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840582f4-3e44-4424-9efa-75bc2a3387c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "# Parameters:\n",
    "#   - model_name_or_path: HuggingFace model identifier (must match key)\n",
    "#   - device: \"cuda\" for GPU, \"cpu\" for CPU (much slower)\n",
    "#   - torch_dtype: \"bf16\" for bfloat16 precision (saves memory, matches key)\n",
    "\n",
    "MODEL_NAME = key_data[\"model_name_or_path\"]  # Use the model specified in the key\n",
    "DEVICE = \"cuda\"  # Change to \"cpu\" if no GPU available\n",
    "DTYPE = key_data.get(\"torch_dtype\", \"bf16\")\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(f\"Device: {DEVICE}, Dtype: {DTYPE}\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "tokenizer, model = subtext_codec.load_model_and_tokenizer(\n",
    "    MODEL_NAME,\n",
    "    DEVICE,\n",
    "    DTYPE\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Vocabulary size: {len(tokenizer):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encode-section",
   "metadata": {},
   "source": [
    "## 4. Encode New Data\n",
    "\n",
    "Now let's encode our own secret message. We'll create a new configuration and\n",
    "encode a custom payload into fresh steganographic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "create-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Configuration:\n",
      "  Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "  Prompt: 'Here's an interesting fact about neural networks: '\n",
      "  top_k: 16\n",
      "  top_p: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Create a codec configuration for encoding\n",
    "# These parameters control how data is hidden in the generated text\n",
    "\n",
    "encode_config = subtext_codec.CodecConfig(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    device=DEVICE,\n",
    "    \n",
    "    # The prompt that starts the generated text\n",
    "    # Choose something that gives the model context for natural generation\n",
    "    prompt_prefix=\"Here's an interesting fact about neural networks: \",\n",
    "    \n",
    "    # top_k: Maximum number of candidate tokens at each step\n",
    "    # Higher = more capacity per token, but potentially less natural text\n",
    "    top_k=16,\n",
    "    \n",
    "    # top_p: Nucleus sampling threshold (0.0 to 1.0)\n",
    "    # Only tokens within this cumulative probability mass are considered\n",
    "    # Lower = more focused/natural text, less encoding capacity\n",
    "    top_p=0.9,\n",
    "    \n",
    "    # Store model info in the key for easier decoding later\n",
    "    store_model_in_key=True,\n",
    "    \n",
    "    torch_dtype=DTYPE,\n",
    ")\n",
    "\n",
    "print(\"Encoding Configuration:\")\n",
    "print(f\"  Model: {encode_config.model_name_or_path}\")\n",
    "print(f\"  Prompt: '{encode_config.prompt_prefix}'\")\n",
    "print(f\"  top_k: {encode_config.top_k}\")\n",
    "print(f\"  top_p: {encode_config.top_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "encode-custom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload to encode: 70 bytes\n",
      "Content: This is my secret message! It will be hidden inside AI-generated text.\n",
      "\n",
      "Encoding... (this generates text token by token)\n",
      "\n",
      "Generated 993 characters of steganographic text\n",
      "\n",
      "--- Begin Encoded Text ---\n",
      "\n",
      "Here's an interesting fact about neural networks: 80% to a neural network can learn in the first 8-10 training cycles and then, in essence, they are stuck at about the 80% level, with little to know increase in learning, despite increasing training cycles.\n",
      "The above is from a 1982 report from Stanford on the subject, where David A. Bayer was experimenting on this particular phenomenon. The full quote goes: \n",
      "``We've seen in all experiments to date that at around 80% or 90%, there will come a plateau where, for no good apparent reason, learning will suddenly cease.\" \n",
      "\n",
      "It appears the 80-20 principle is very much applicable to this field and other fields like finance and even software, where about 20% effort generates 80-80-80 of the results.\n",
      "\n",
      "I would also like the comment on what might happen when 1.4 or 5 billion neurons can learn faster than our brains, in terms of learning capacity or intelligence.\n",
      "\n",
      "Commenting on this topic is not a question or problem to be answered by me, itâ€™s\n",
      "\n",
      "--- End Encoded Text ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a custom secret payload to encode\n",
    "my_secret = b\"This is my secret message! It will be hidden inside AI-generated text.\"\n",
    "\n",
    "print(f\"Payload to encode: {len(my_secret)} bytes\")\n",
    "print(f\"Content: {my_secret.decode('utf-8')}\")\n",
    "print(\"\\nEncoding... (this generates text token by token)\\n\")\n",
    "\n",
    "# Encode the data into steganographic text\n",
    "# Returns:\n",
    "#   - encoded_text: The generated text with hidden data\n",
    "#   - new_key: CodecKey needed to decode the message later\n",
    "encoded_text, new_key = subtext_codec.encode_data_to_text(\n",
    "    data=my_secret,\n",
    "    cfg=encode_config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(encoded_text)} characters of steganographic text\")\n",
    "print(f\"\\n--- Begin Encoded Text ---\\n\")\n",
    "print(encoded_text)\n",
    "print(f\"\\n--- End Encoded Text ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "show-new-key",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Codec Key ---\n",
      "\n",
      "  version: v2\n",
      "  top_k: 16\n",
      "  top_p: 0.9\n",
      "  payload_length: 70\n",
      "  prompt_prefix: 'Here's an interesting fact about neural networks: '\n",
      "  model_name_or_path: meta-llama/Llama-3.1-8B-Instruct\n",
      "\n",
      "IMPORTANT: Without this key, the message cannot be decoded!\n"
     ]
    }
   ],
   "source": [
    "# Examine the generated key\n",
    "# This key is REQUIRED to decode the message - keep it safe!\n",
    "\n",
    "print(\"--- Generated Codec Key ---\\n\")\n",
    "print(f\"  version: {new_key.version}\")\n",
    "print(f\"  top_k: {new_key.top_k}\")\n",
    "print(f\"  top_p: {new_key.top_p}\")\n",
    "print(f\"  payload_length: {new_key.payload_length}\")\n",
    "print(f\"  prompt_prefix: '{new_key.prompt_prefix}'\")\n",
    "print(f\"  model_name_or_path: {new_key.model_name_or_path}\")\n",
    "\n",
    "print(\"\\nIMPORTANT: Without this key, the message cannot be decoded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "roundtrip-verify",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing round-trip verification...\n",
      "\n",
      "Round-trip verification PASSED!\n",
      "  Original:  b'This is my secret message! It will be hidden inside AI-generated text.'\n",
      "  Decoded:   b'This is my secret message! It will be hidden inside AI-generated text.'\n",
      "  Match:     True\n"
     ]
    }
   ],
   "source": [
    "# Verify round-trip: decode the text we just encoded\n",
    "# This confirms encoding -> decoding recovers the exact original data\n",
    "\n",
    "print(\"Performing round-trip verification...\\n\")\n",
    "\n",
    "roundtrip_decoded = subtext_codec.decode_text_to_data(\n",
    "    encoded_text=encoded_text,\n",
    "    key=new_key,\n",
    "    prompt_prefix=encode_config.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Verify exact match\n",
    "assert roundtrip_decoded == my_secret, \"Round-trip verification failed!\"\n",
    "\n",
    "print(\"Round-trip verification PASSED!\")\n",
    "print(f\"  Original:  {my_secret}\")\n",
    "print(f\"  Decoded:   {roundtrip_decoded}\")\n",
    "print(f\"  Match:     {roundtrip_decoded == my_secret}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robustness-section",
   "metadata": {},
   "source": [
    "## 5. Robustness: Handling Leading/Trailing Text\n",
    "\n",
    "The decoder is robust to leading/trailing text after the encoded message. This is useful because:\n",
    "- The encoded text might be copied with extra content\n",
    "- Social media or messaging apps might append signatures or metadata\n",
    "\n",
    "The decoder uses a **sentinel token** to know where the encoded data ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "test-trailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original encoded length: 993 chars\n",
      "With leading/trailing noise:     1148 chars\n",
      "\n",
      "Decoding noisy text...\n",
      "\n",
      "SUCCESS: Decoded correctly despite trailing noise!\n",
      "Recovered: This is my secret message! It will be hidden inside AI-generated text.\n"
     ]
    }
   ],
   "source": [
    "# Add some leading and trailing text that wasn't part of the original encoding\n",
    "noisy_text = \"\\n\\n[This leading text was added later and is not part of the encoded message.]\" + encoded_text + \"\\n\\n[This trailing text was added later and is not part of the encoded message.]\"\n",
    "\n",
    "print(f\"Original encoded length: {len(encoded_text)} chars\")\n",
    "print(f\"With leading/trailing noise:     {len(noisy_text)} chars\")\n",
    "print(f\"\\nDecoding noisy text...\\n\")\n",
    "\n",
    "# Decode should still work, ignoring the trailing content\n",
    "decoded_from_noisy = subtext_codec.decode_text_to_data(\n",
    "    encoded_text=noisy_text,\n",
    "    key=new_key,\n",
    "    prompt_prefix=encode_config.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "assert decoded_from_noisy == my_secret, \"Decoding with trailing text failed!\"\n",
    "\n",
    "print(\"SUCCESS: Decoded correctly despite trailing noise!\")\n",
    "print(f\"Recovered: {decoded_from_noisy.decode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-section",
   "metadata": {},
   "source": [
    "## 6. Saving and Loading Keys\n",
    "\n",
    "In practice, you'll want to save the codec key to share with the intended recipient.\n",
    "The key is essential for decoding - treat it like a password!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the key to a file\n",
    "output_key_path = Path(\"my_key.json\")\n",
    "\n",
    "subtext_codec.save_codec_key(new_key, output_key_path)\n",
    "\n",
    "print(f\"Key saved to: {output_key_path}\")\n",
    "print(f\"\\nKey file contents:\")\n",
    "print(output_key_path.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the key back and verify it works\n",
    "loaded_key = subtext_codec.load_codec_key(output_key_path)\n",
    "\n",
    "# Decode using the loaded key\n",
    "decoded_with_loaded_key = subtext_codec.decode_text_to_data(\n",
    "    text=encoded_text,\n",
    "    key=loaded_key,\n",
    "    prompt_prefix=encode_config.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "assert decoded_with_loaded_key == my_secret\n",
    "print(\"Successfully decoded using loaded key!\")\n",
    "\n",
    "# Clean up the temporary key file\n",
    "output_key_path.unlink()\n",
    "print(\"(Temporary key file cleaned up)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-section",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo covered the core functionality of subtext-codec:\n",
    "\n",
    "1. **Environment Setup**: Configure CUDA determinism and set random seeds\n",
    "2. **Sample Data**: Explored the provided samples (secret, key, message)\n",
    "3. **Model Loading**: Load the language model required for encoding/decoding\n",
    "4. **Decoding**: Recover hidden data from steganographic text using a key\n",
    "5. **Encoding**: Hide arbitrary binary data inside natural-looking LLM text\n",
    "6. **Robustness**: The decoder handles trailing text gracefully\n",
    "7. **Key Management**: Save and load codec keys for later decoding\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Determinism is critical**: Both encoding and decoding must use the same model,\n",
    "  parameters, and random seed to work correctly.\n",
    "  \n",
    "- **The key is essential**: Without the codec key, the message cannot be decoded.\n",
    "  The key contains the prompt prefix, top_k/top_p settings, and payload length.\n",
    "  \n",
    "- **Capacity vs. naturalness**: Higher top_k allows more data per token but may\n",
    "  produce less natural text. Adjust based on your needs.\n",
    "  \n",
    "- **Model consistency**: The exact same model (including version and precision)\n",
    "  must be used for both encoding and decoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
