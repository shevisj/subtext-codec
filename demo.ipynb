{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ac67d",
   "metadata": {},
   "source": [
    "# Subtext Codec Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b841813e-11d8-4b9f-8000-a3cbc08a6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83062502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subtext_codec import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3806749e-079a-4f34-9dc9-bff1fbb435b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subtext_codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ae8d8d-41a7-43fa-a3c6-b8ad9e0d7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8216ef8b-bcf9-4582-9e4e-d7367c5686c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdafd194dfea46f2bb68fe40a9399d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f00033c-68a7-485c-b238-7e3ac8076162",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtext_codec.set_deterministic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840582f4-3e44-4424-9efa-75bc2a3387c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find LlamaForCausalLM neither in <module 'transformers.models.llama' from 'C:\\\\Users\\\\shevi\\\\src\\\\github.com\\\\shevisj\\\\subtext-codec\\\\.venv\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\llama\\\\__init__.py'> nor in <module 'transformers' from 'C:\\\\Users\\\\shevi\\\\src\\\\github.com\\\\shevisj\\\\subtext-codec\\\\.venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:741\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:745\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find LlamaForCausalLM in <module 'transformers' from 'C:\\\\Users\\\\shevi\\\\src\\\\github.com\\\\shevisj\\\\subtext-codec\\\\.venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer, model = \u001b[43msubtext_codec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.1-8B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\subtext_codec\\codec.py:167\u001b[39m, in \u001b[36mload_model_and_tokenizer\u001b[39m\u001b[34m(model_name_or_path, device, torch_dtype)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    166\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m] = resolved_dtype\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m model.to(device)\n\u001b[32m    169\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:601\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    598\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    599\u001b[39m     )\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     model_class = \u001b[43m_get_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:394\u001b[39m, in \u001b[36m_get_model_class\u001b[39m\u001b[34m(config, model_mapping)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     supported_models = \u001b[43mmodel_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    395\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(supported_models, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m supported_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:807\u001b[39m, in \u001b[36m_LazyAutoMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_mapping:\n\u001b[32m    806\u001b[39m     model_name = \u001b[38;5;28mself\u001b[39m._model_mapping[model_type]\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_attr_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[32m    810\u001b[39m model_types = [k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_mapping.items() \u001b[38;5;28;01mif\u001b[39;00m v == key.\u001b[34m__name__\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:821\u001b[39m, in \u001b[36m_LazyAutoMapping._load_attr_from_module\u001b[39m\u001b[34m(self, model_type, attr)\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m    820\u001b[39m     \u001b[38;5;28mself\u001b[39m._modules[module_name] = importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtransformers.models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetattribute_from_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:743\u001b[39m, in \u001b[36mgetattribute_from_module\u001b[39m\u001b[34m(module, attr)\u001b[39m\n\u001b[32m    741\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m getattribute_from_module(transformers_module, attr)\n\u001b[32m    742\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m neither in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nor in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Could not find LlamaForCausalLM neither in <module 'transformers.models.llama' from 'C:\\\\Users\\\\shevi\\\\src\\\\github.com\\\\shevisj\\\\subtext-codec\\\\.venv\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\llama\\\\__init__.py'> nor in <module 'transformers' from 'C:\\\\Users\\\\shevi\\\\src\\\\github.com\\\\shevisj\\\\subtext-codec\\\\.venv\\\\Lib\\\\site-packages\\\\transformers\\\\__init__.py'>!"
     ]
    }
   ],
   "source": [
    "tokenizer, model = subtext_codec.load_model_and_tokenizer(\n",
    "    \"meta-llama/Llama-3.1-8B\", \"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae2c94-8ab9-45b6-9fc2-a2072e0d9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = subtext_codec.CodecConfig(\n",
    "    model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device=\"cuda\",\n",
    "    base=3,\n",
    "    prompt_prefix=\"In response to a video about an adorable cat playing with a leaf: \",\n",
    "    max_context_length=None,\n",
    "    top_k=8,\n",
    "    store_model_in_key=True,\n",
    "    torch_dtype=\"bf16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd98ad50-8744-44d7-bf03-8c788f4dbfaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reached max_new_tokens=128 before consuming all digits (128/146)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m payload = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhello world my name is shevis\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoded, key = \u001b[43msubtext_codec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_data_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m key.model_name_or_path == cfg.model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\subtext_codec\\codec.py:195\u001b[39m, in \u001b[36mencode_data_to_text\u001b[39m\u001b[34m(data, cfg, model, tokenizer)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m digit_idx < \u001b[38;5;28mlen\u001b[39m(digits):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_ids.shape[\u001b[32m1\u001b[39m] - prompt_ids.shape[\u001b[32m1\u001b[39m] >= cfg.max_new_tokens:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    196\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReached max_new_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.max_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m before consuming all digits \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdigit_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(digits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         )\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    201\u001b[39m         outputs = model(input_ids=input_ids)\n",
      "\u001b[31mValueError\u001b[39m: Reached max_new_tokens=128 before consuming all digits (128/146)"
     ]
    }
   ],
   "source": [
    "payload = b\"hello world my name is shevis\"\n",
    "encoded, key = subtext_codec.encode_data_to_text(payload, cfg, model, tokenizer)\n",
    "assert key.model_name_or_path == cfg.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ba835-c883-4f37-a5a0-ae30bcf9b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086ec39-669e-4ef5-8d49-2c89820ce2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = subtext_codec.decode_text_to_data(\n",
    "    encoded,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")\n",
    "assert decoded == payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6ff85e-2d09-4595-ac1b-5f4194b9dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, 20th Century Fox had the idea of creating an all-fantasy, live-action movie franchise, which would have included a movie called “Rapaz”, about the legendary King Kong, “Tiger King,” a movie adaptation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ff21c-4054-45f6-ae81-db69ae788fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_encoded = encoded + \" Trailing unrelated text after sentinel.\"\n",
    "decoded_with_noise = subtext_codec.decode_text_to_data(\n",
    "    noisy_encoded,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cpu\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")\n",
    "assert decoded_with_noise == payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35f500-b0d6-46e1-88ae-e09a7e2f6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ids = tokenizer(encoded, return_tensors=\"pt\").input_ids[0][:-1]\n",
    "trimmed_text = tokenizer.decode(trimmed_ids, skip_special_tokens=True)\n",
    "subtext_codec.decode_text_to_data(\n",
    "    trimmed_text,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
