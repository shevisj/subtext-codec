{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82ac67d",
   "metadata": {},
   "source": [
    "# Subtext Codec Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b841813e-11d8-4b9f-8000-a3cbc08a6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
     ]
    }
   ],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83062502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subtext_codec import cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3806749e-079a-4f34-9dc9-bff1fbb435b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subtext_codec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7ae8d8d-41a7-43fa-a3c6-b8ad9e0d7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8216ef8b-bcf9-4582-9e4e-d7367c5686c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236407dd1a2d461ba362922ca8abbe69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f00033c-68a7-485c-b238-7e3ac8076162",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtext_codec.set_deterministic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "840582f4-3e44-4424-9efa-75bc2a3387c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c8f63a99634b75882035e2a9d4518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shevi\\src\\github.com\\shevisj\\subtext-codec\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shevi\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bb655cfc874d28b03eee737e09a66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a3b44df7e944a0b275d76559c5f8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ac1dfb12f44fce92050cc88297a9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f684a04da6d4853a41b54aaa72ba754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38abb4d0f619444badcd4fa6a81c38fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bfc5e501e44f20b913ba20dbd3bee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170e24e2f7024e57bf7c4c99d7670160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5330e2e574e94ba58a63a8016222eb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969d8e0568464316894277c01242e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9113e0f151fd429083e697b77871f63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fe88e92dc94b00b243382736649128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer, model = subtext_codec.load_model_and_tokenizer(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", \"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59ae2c94-8ab9-45b6-9fc2-a2072e0d9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = subtext_codec.CodecConfig(\n",
    "    model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device=\"cuda\",\n",
    "    base=3,\n",
    "    prompt_prefix=\"In response to a video about an adorable cat playing with a leaf: \",\n",
    "    max_new_tokens=512,\n",
    "    max_context_length=None,\n",
    "    top_k=8,\n",
    "    store_model_in_key=True,\n",
    "    torch_dtype=\"bf16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd98ad50-8744-44d7-bf03-8c788f4dbfaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Reached max_new_tokens=128 before consuming all digits (128/146)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m payload = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhello world my name is shevis\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoded, key = \u001b[43msubtext_codec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_data_to_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m key.model_name_or_path == cfg.model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\src\\github.com\\shevisj\\subtext-codec\\subtext_codec\\codec.py:195\u001b[39m, in \u001b[36mencode_data_to_text\u001b[39m\u001b[34m(data, cfg, model, tokenizer)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m digit_idx < \u001b[38;5;28mlen\u001b[39m(digits):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m input_ids.shape[\u001b[32m1\u001b[39m] - prompt_ids.shape[\u001b[32m1\u001b[39m] >= cfg.max_new_tokens:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    196\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReached max_new_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.max_new_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m before consuming all digits \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdigit_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(digits)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         )\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    201\u001b[39m         outputs = model(input_ids=input_ids)\n",
      "\u001b[31mValueError\u001b[39m: Reached max_new_tokens=128 before consuming all digits (128/146)"
     ]
    }
   ],
   "source": [
    "payload = b\"hello world my name is shevis\"\n",
    "encoded, key = subtext_codec.encode_data_to_text(payload, cfg, model, tokenizer)\n",
    "assert key.model_name_or_path == cfg.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39ba835-c883-4f37-a5a0-ae30bcf9b7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086ec39-669e-4ef5-8d49-2c89820ce2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = subtext_codec.decode_text_to_data(\n",
    "    encoded,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")\n",
    "assert decoded == payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6ff85e-2d09-4595-ac1b-5f4194b9dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, 20th Century Fox had the idea of creating an all-fantasy, live-action movie franchise, which would have included a movie called “Rapaz”, about the legendary King Kong, “Tiger King,” a movie adaptation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ff21c-4054-45f6-ae81-db69ae788fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_encoded = encoded + \" Trailing unrelated text after sentinel.\"\n",
    "decoded_with_noise = subtext_codec.decode_text_to_data(\n",
    "    noisy_encoded,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cpu\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")\n",
    "assert decoded_with_noise == payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35f500-b0d6-46e1-88ae-e09a7e2f6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_ids = tokenizer(encoded, return_tensors=\"pt\").input_ids[0][:-1]\n",
    "trimmed_text = tokenizer.decode(trimmed_ids, skip_special_tokens=True)\n",
    "subtext_codec.decode_text_to_data(\n",
    "    trimmed_text,\n",
    "    key=key,\n",
    "    prompt_prefix=cfg.prompt_prefix,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda\",\n",
    "    max_context_length=cfg.max_context_length,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
